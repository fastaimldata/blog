{
  
    
        "post0": {
            "title": "Data Engineering Bootcamp Weeks 1-2 (Ongoing)",
            "content": "Data Engineering Weeks 1-2 (Ongoing) . The objective of this post is to document my learning journey while taking the data engineering zoomcamp. I use Windows+WSL for Linux support. I will defer from posting installation related issues here, unless it seems helpful either for reference in the future or to the other course takers. . Notes for other learners . If you are taking the data engineering zoomcamp, please follow the course at the official zoomcamp repository. The course creators have done a nice job in compiling this step by step. | Please use the notes here only as a complementary or additional resource | You can ask questions in the channel #course-data-engineering in DataTalks.Club | This is an innovative and community based live course provided by some kind-hearted people for free. A good way to contribute back is to document the issues you face on the official FAQ page | . Week 1 - Docker . Docker is a to package applications into containers which can be deployed/shipped anywhere. Docker achieves this through OS virtualization technology. Each docker container has its own OS, libraries and configuration files in an isolated space. But docker containers can colloborate with each other though channels. Let us say we have built a python application which runs on our local windows machine. If we containerize this application, then we can take the exact same container and make it run on another machine running another OS and on the cloud. I have already used a fair amount of docker. Therefore, it was easy to pick up. . In this course, docker is chosen so that the data pipeline we are going to develop can be containerized. Then, this containerized pipeline can be deployed on the cloud with services like AWS Batch or Kubernetes jobs. The Spark and Serverless Technologies(AWS Lambda, Google Functions) explored in this course are all containerized. . The following exercises were performed hands-on during this week . Setup containerized postgres with persistence on local disk . We will later ingest New York taxis dataset into a postgres database. For persistence, I mapped an empty local folder ny_taxi_postgres_data to /var/lib/postgresql/data and this ran into permission issues because the released postgres container has only user “postgres”. I tried some suggested workarounds and finally settled on this one - we can create a local docker volume and map it to postgres data folder to avoid the permissions issue. . docker volume create --name dtc_postgres_volume_local -d local . docker run -it -e POSTGRES_USER=&quot;root&quot; -e POSTGRES_PASSWORD=&quot;root&quot; -e POSTGRES_DB=&quot;ny_taxi&quot; -v dtc_postgres_volume_local:/var/lib/postgresql/data -p 5432:5432 postgres:13 . Containerize python application . I built a simple docker image of a python application which uses pandas. . In fact, we can collect all python requirements into a requirements.txt file which can be used by pip on the container to prepare python environments in a clean way. . Week 2 - Terraform and GCP . Terraform . Terraform is an open source tool which allows to handle “Infrastructure-as-Code”. For real beginners, By infrastructure, we mean not only computing infratructure (AWS EC2, Google Cloud machines) but also other resources like databases. If this does not make sense to you, please read this page . | Where? Terraform allows to provision infrastructure locally or on the cloud with various cloud providers like AWS, GCP etc. This provisioning of infrastructure can be done for testing, production etc. In Terraform, Providers are plugins that implement resource types. Docker, AWS, Google Cloud, Alibaba Cloud are all providers. . | How? Using configuration files which are source-controlled. This approach helps to keep track of what infrastructure we used for which purpose in a clean and efficient manner. Moreover, Hashicorp, the company behind Terraform provides us with small, reusable Terraform configurations called Modules. We can use these to manage a group of related resources as if they were a single resource. . | I recommend doing the official Terraform docker Interactive Lab in which terraform uses docker as a provider. The resources provided are a docker image and docker container. No installations are required to follow this lab. . GCP . This course introduces GCP as a provider which will provide us with the following 2 resources: . Data Lake - Google Cloud Storage(or GCS) | Data Warehouse - BigQuery | Data Warehouse vs Data Lake . This is essentially ETL vs ELT. Data warehousing has existed for a long time. It uses structured data, relational and clean in the order of Terabytes. Business Analysts use data warehouses for specific use-cases like BI, reporting and batch processing. Data Lakes came about later taking into account the fact that businesses wanted data to be made immediately useful, as soon as it is available. So datalakes can have unstructured, semi-structured or structured data to the order of even Petabytes. The use cases are for streaming analytics, machine learning and AI. . E=Extract, T=Transform, L=Load. ETL is for small amounts of data or Schema on Write approach - a schema is defined, the relationshipa are clearly defined and then the data is written. ELT on the other hand is a Schema on Read approach where data is written first and we worry about schema when reading data. This approach provides support for data lakes. . Provision datalake and datawarehousing with Terraform on Google Cloud Platform . The objective of the homework is to provision a datalake and a datawarehouse on Google Cloud Platform using Terraform. . First, on the google cloud platform (300$ free credits), it is necessary to setup . A new goole cloud project (and note down its project ID) | A new service account (through which to access the services provided by google cloud) and download this account’s credentials as a json file and | Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to this location $ export GOOGLE_APPLICATION_CREDENTIALS=&quot;&lt;path/to/your/service-account-authkeys&gt;.json&quot; | $ cd /mnt/Projects/data-engineering-zoomcamp/week_1_basics_n_setup/1_terraform_gcp/terraform . Here, we have two files main.tf and variables.tf. In variables.tf, we can set the google cloud project ID from the previous step . $ terraform init . terraform init Initializing the backend... Initializing provider plugins... - Reusing previous version of hashicorp/google from the dependency lock file - Using previously-installed hashicorp/google v4.8.0 Terraform has been successfully initialized! You may now begin working with Terraform. Try running &quot;terraform plan&quot; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. . $ terraform plan . Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_bigquery_dataset.dataset will be created + resource &quot;google_bigquery_dataset&quot; &quot;dataset&quot; { + creation_time = (known after apply) + dataset_id = &quot;trips_data_all&quot; + delete_contents_on_destroy = false + etag = (known after apply) + id = (known after apply) + last_modified_time = (known after apply) + location = &quot;europe-west6&quot; + project = &quot;fast-pagoda-339723&quot; + self_link = (known after apply) + access { + domain = (known after apply) + group_by_email = (known after apply) + role = (known after apply) + special_group = (known after apply) + user_by_email = (known after apply) + view { + dataset_id = (known after apply) + project_id = (known after apply) + table_id = (known after apply) } } } # google_storage_bucket.data-lake-bucket will be created + resource &quot;google_storage_bucket&quot; &quot;data-lake-bucket&quot; { + force_destroy = true + id = (known after apply) + location = &quot;EUROPE-WEST6&quot; + name = &quot;dtc_data_lake_fast-pagoda-339723&quot; + project = (known after apply) + self_link = (known after apply) + storage_class = &quot;STANDARD&quot; + uniform_bucket_level_access = true + url = (known after apply) + lifecycle_rule { + action { + type = &quot;Delete&quot; } + condition { + age = 30 + matches_storage_class = [] + with_state = (known after apply) } } + versioning { + enabled = true } } Plan: 2 to add, 0 to change, 0 to destroy. ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Note: You didn&#39;t use the -out option to save this plan, so Terraform can&#39;t guarantee to take exactly these actions if you run &quot;terraform apply&quot; now. . $ terraform apply . Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_bigquery_dataset.dataset will be created + resource &quot;google_bigquery_dataset&quot; &quot;dataset&quot; { + creation_time = (known after apply) + dataset_id = &quot;trips_data_all&quot; + delete_contents_on_destroy = false + etag = (known after apply) + id = (known after apply) + last_modified_time = (known after apply) + location = &quot;europe-west6&quot; + project = &quot;fast-pagoda-339723&quot; + self_link = (known after apply) + access { + domain = (known after apply) + group_by_email = (known after apply) + role = (known after apply) + special_group = (known after apply) + user_by_email = (known after apply) + view { + dataset_id = (known after apply) + project_id = (known after apply) + table_id = (known after apply) } } } # google_storage_bucket.data-lake-bucket will be created + resource &quot;google_storage_bucket&quot; &quot;data-lake-bucket&quot; { + force_destroy = true + id = (known after apply) + location = &quot;EUROPE-WEST6&quot; + name = &quot;dtc_data_lake_fast-pagoda-339723&quot; + project = (known after apply) + self_link = (known after apply) + storage_class = &quot;STANDARD&quot; + uniform_bucket_level_access = true + url = (known after apply) + lifecycle_rule { + action { + type = &quot;Delete&quot; } + condition { + age = 30 + matches_storage_class = [] + with_state = (known after apply) } } + versioning { + enabled = true } } Plan: 2 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only &#39;yes&#39; will be accepted to approve. Enter a value: yes google_bigquery_dataset.dataset: Creating... google_storage_bucket.data-lake-bucket: Creating... google_storage_bucket.data-lake-bucket: Creation complete after 1s [id=dtc_data_lake_fast-pagoda-339723] google_bigquery_dataset.dataset: Creation complete after 1s [id=projects/fast-pagoda-339723/datasets/trips_data_all] . $ terraform destroy . google_storage_bucket.data-lake-bucket: Refreshing state... [id=dtc_data_lake_fast-pagoda-339723] google_bigquery_dataset.dataset: Refreshing state... [id=projects/fast-pagoda-339723/datasets/trips_data_all] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_bigquery_dataset.dataset will be destroyed - resource &quot;google_bigquery_dataset&quot; &quot;dataset&quot; { - creation_time = 1643547397011 -&gt; null - dataset_id = &quot;trips_data_all&quot; -&gt; null - default_partition_expiration_ms = 0 -&gt; null - default_table_expiration_ms = 0 -&gt; null - delete_contents_on_destroy = false -&gt; null - etag = &quot;uSEBYtpaZBci9jzHBKd49A==&quot; -&gt; null - id = &quot;projects/fast-pagoda-339723/datasets/trips_data_all&quot; -&gt; null - labels = {} -&gt; null - last_modified_time = 1643547397011 -&gt; null - location = &quot;europe-west6&quot; -&gt; null - project = &quot;fast-pagoda-339723&quot; -&gt; null - self_link = &quot;https://bigquery.googleapis.com/bigquery/v2/projects/fast-pagoda-339723/datasets/trips_data_all&quot; -&gt; null - access { - role = &quot;OWNER&quot; -&gt; null - user_by_email = &quot;data-engineer-service-account@fast-pagoda-339723.iam.gserviceaccount.com&quot; -&gt; null } - access { - role = &quot;OWNER&quot; -&gt; null - special_group = &quot;projectOwners&quot; -&gt; null } - access { - role = &quot;READER&quot; -&gt; null - special_group = &quot;projectReaders&quot; -&gt; null } - access { - role = &quot;WRITER&quot; -&gt; null - special_group = &quot;projectWriters&quot; -&gt; null } } # google_storage_bucket.data-lake-bucket will be destroyed - resource &quot;google_storage_bucket&quot; &quot;data-lake-bucket&quot; { - default_event_based_hold = false -&gt; null - force_destroy = true -&gt; null - id = &quot;dtc_data_lake_fast-pagoda-339723&quot; -&gt; null - labels = {} -&gt; null - location = &quot;EUROPE-WEST6&quot; -&gt; null - name = &quot;dtc_data_lake_fast-pagoda-339723&quot; -&gt; null - project = &quot;fast-pagoda-339723&quot; -&gt; null - requester_pays = false -&gt; null - self_link = &quot;https://www.googleapis.com/storage/v1/b/dtc_data_lake_fast-pagoda-339723&quot; -&gt; null - storage_class = &quot;STANDARD&quot; -&gt; null - uniform_bucket_level_access = true -&gt; null - url = &quot;gs://dtc_data_lake_fast-pagoda-339723&quot; -&gt; null - lifecycle_rule { - action { - type = &quot;Delete&quot; -&gt; null } - condition { - age = 30 -&gt; null - days_since_custom_time = 0 -&gt; null - days_since_noncurrent_time = 0 -&gt; null - matches_storage_class = [] -&gt; null - num_newer_versions = 0 -&gt; null - with_state = &quot;ANY&quot; -&gt; null } } - versioning { - enabled = true -&gt; null } } Plan: 0 to add, 0 to change, 2 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only &#39;yes&#39; will be accepted to confirm. Enter a value: yes google_storage_bucket.data-lake-bucket: Destroying... [id=dtc_data_lake_fast-pagoda-339723] google_bigquery_dataset.dataset: Destroying... [id=projects/fast-pagoda-339723/datasets/trips_data_all] google_storage_bucket.data-lake-bucket: Destruction complete after 1s google_bigquery_dataset.dataset: Destruction complete after 1s Destroy complete! Resources: 2 destroyed. .",
            "url": "https://fastaimldata.github.io/blog/courses/data%20engineering/2022/01/29/data-engineering-zoomcamp.html",
            "relUrl": "/courses/data%20engineering/2022/01/29/data-engineering-zoomcamp.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fairness in Machine Learning",
            "content": "Objectives . The objective of this post is two-fold: . Concept Explanations - I wish to explain the main concepts in fairness in very simple and practical terms. For the technically inclined, I would like to provide some depth using mathematics borrowed from Moritz Hardt et al., 2016 . | Learning materials - Record here all the resources which I found useful while delving on the topic of fairness in machine learning . | Concepts . Sensitive attributes . | Demographic parity . | Equal odds . | Equal opportunity . | Fairness through Awareness . | Resources . Attacking discrimination in ML . | Equality of opportunity in ML . | Duke slides . | Fair ML book . | Moritz Hardt et al., Equality of opportunity in supervised learning . |",
            "url": "https://fastaimldata.github.io/blog/machine%20learning/2022/01/22/fainess-in-machine-learning.html",
            "relUrl": "/machine%20learning/2022/01/22/fainess-in-machine-learning.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Coursera GAN specialization (Ongoing)",
            "content": "Motivations . This post is to document my learning journey on GANs or Generative Adversarial Networks. GAN technology is considered to be one of the most promising advancements in the field of AI. I have worked in facial recognition previously and I have a decent background in ML/deep learning. Anyone who has trained deep learning models would understand the importance of training data, which is scarce in many cases. . My learning goals are the following: . Obviously, for the pleasure of learning! First. | To keep myself up-to-date with deep learning advancements. | Hopefully, I would like to explore how much GANs can actually help in synthetic data generation for training deep learning models. | Perhaps see if I can do something creative with these GANs. | Rather than scouring the web for resources randomly, I want to take a more principled approach. I decide to follow the GAN Specialization on Coursera. . I completed the wonderful Machine Learning course taught by Prof. Andew Ng a few years ago and I loved it. I did all the exercises sincerely and learnt a lot from it. I even had the Ecole Doctorale (the PhD program office at my French university) to accept this toward my course requirements. I hope to derive the same pleasure and value from this course. . To really motivate myself, I have taken the paid option with Coursera which costs 40 EUR/month. I asked if my company can take care of this budget but I did not get a nod. Actually I don’t care and I think this is a good investment. . Coursera GAN specialization . There are a total of 3 courses in this specialization. Each course has a number of programming assignments to be completed. I like this do-and-learn approach in general which for me is quite effective strategy. . I hope to review each of the 3 courses here as and when I progress through the course. This is an additional motivator for me. . Course1 : Build Basic Generative Adversarial Networks . The lectures were sequenced nicely with crisp and clear explanations. . | The coding assignments could have been a little more harder. They did not challenge me at all. Probably, this was intended by the course creators since too hard assignments might result in students dropping out of the course or the specialization itself. It is well-known that the dropout rates are extremely high in MOOC courses. Perhaps, another reason is that ML/deep learning is not something very new to me. . | . Course2 : Build Better Generative Adversarial Networks . A lot of optional python notebooks were provided which I found positive. . | Yet again, the programming assignment was boring. I felt as though this was conceived in a hurry. I could simply fill up the missing code snippets as if putting back a puzzle together and get away scot-free! . | I completely agree that ethics and fairness in machine learning are extremely important and relevant. Unfortunately, the explanations provided for fairness concepts including the ones in the Bias quiz on Week2 were inadequate. I can go a step further and say that the clarity and fluidity in the progress of Course 1 was missing in Course 2. . | In the quiz, instead of flying elephants and flamingoes when discussing equality of opportunity, we could have introduced a real-world example, like gender bias in hiring decisions or loan application acceptances or college admissions. In one sentence, your intentions were noble but the execution could have been really better. . | . My general comments to the course creators . Sacrifice depth for width - There are so many additional optional readings (like in week3) where you have simply suggested papers to read. In my opinion, this could be replaced with in-depth discussions. As example is to discuss about the actual training in the assignment notebooks. . | My suggestion would be to add a project or a list of projects - this should be a real problem which can be solved by GANs. In the courses, you can build those concepts which will be eventually necessary to solve this problem and complete the project. . | As ML engineers and practitioners, we are interested in knowing how to solve problems which show up in practice during the training. For example, how to diagnose why a training has failed, how to log experiments (online or offline wandb integration) and what are the remedies for it. . | Please publish the statistics on number of people who took the course and the average time spent by them to complete the videos and assignments in each week for each course. This might help us to be organized and plan our schedules accordingly. . |",
            "url": "https://fastaimldata.github.io/blog/courses/deep%20learning/2022/01/08/coursera-gan-specialization.html",
            "relUrl": "/courses/deep%20learning/2022/01/08/coursera-gan-specialization.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fastaimldata.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://fastaimldata.github.io/blog/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an R&amp;D engineer by profession. My work spans machine learning, deep learning, software engineering and a lot of data (data analysis, visualization, data science) and MLOps. . You can contact me by email iamthedataguy@gmail.com and/or connect with me on Twitter. . Please feel free to reach out to me: . If you are interested in hiring me (or want to discuss about my experience) | If you think I can help you with one of your ML or AI projects | If you think we can colloborate on a specific project | If you have comments or suggestions about the content in this blog |",
          "url": "https://fastaimldata.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fastaimldata.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}